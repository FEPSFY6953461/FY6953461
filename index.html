<!DOCTYPE html>
<html>
<head>

<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">

  
<!-- css -->
  <style>
  .mySlides {display:none;}
  body {
    background-color: #FFFFFF
    margin-left: 50px;
  }
  .ex1 {
    font-family: Calibri, sans-serif;
    width: 50%;
    margin-left: auto;
    margin-right: auto;
  }
  .heading {
    text-align: center;
  }
  .refheading1 {
    margin-left: 550px;
  }
  .refheading2 {
    margin-left: 150px;
  }
  .image {
    margin-left: 650px;
  }
    </style>

</head>
  
  


<body>

  
    <!-- image testing -->
<div class = "image">
<img src="WebBanner2023.png" alt="Banner" width="500" height="250">
</div>
  <div class = "mainheading">
  <h3><b><small>ENG0018 Computer Laboratory 2025-2026</small></b></h3>
  </div>
 
  <h3><small>Student URN: 6953461</small></h3>
  
<hr></hr>

<div class = "mainheading">
<h3 style="font-family:calibri;"><b>Conference Project: What are the limitations of CNN approaches in MRI-based brain diagnosis?</b></h3>
</div>
  
<hr></hr>

   <!-- table of contents -->
  <table>

    <tr>
    <th><h3>Table of contents</h3></th>
    </tr>
    <tr>
    <td><a href="#Abstract">Abstract</td>
    </tr>
    <tr>
    <td><a href="#Introduction">Introduction</td>
    </tr>
    <tr>
    <td><a href="#Discussion">Analysis/Dicussion</td>
    </tr>
    <tr>
    <td><a href="#References">References</td>
    </tr>
  </table>

  <hr></hr>

  
  <!-- abstract -->
<p class = "ex1">
  <div class="heading">
  <h3 id="Abstract">Abstract</h3>
  </div>
  <pre>
    This paper argues that there are several limitations surrounding the use of deep learning models in healthcare systems. Convoluted neural networks are now the most commonly used tool in tumour 
    diagnosis, classification and medical imaging. The article is evaluated on systemic data and figures obtained by private healthcare centres and training data sets from hospitals. 
  </pre>
</p>

<hr></hr>

  <!-- intro -->
  <p class = "ex1">
    <div class="heading">
    <h3 id="Introduction">Introduction</h3>
    </div>
    <pre>
      The aetiology of cancer has always been a topic open to scientific and even philosophical debate. Over the centuries, the medicinal world has seen exponential progress in technological advancements, 
      introducing devices such as X-rays and CT/PET scans which have completely reformed medical imaging and in particular tumour diagnosis. MRI is widely used to identify and classify tumours in the brain 
      and other organs. Deep Learning models have become integrated in most hospitals as studies have proved it helps doctors classify tumours faster and more accurately. However, problems such as insufficient data 
      due to patient privacy and inconsistent model performance across hospitals. This review aims to highlight and critically assess the key limitations in deep learning approaches in MRI-based tumour diagnosis. 
      </pre>
  </p>

  <!-- analysis/discussion -->
  <p class = "ex1">
    <div class = "heading">
      <h3 id="Discussion">Discussion</h3>
    </div>
  <pre>

    <b>2.1 Scarcity of data</b>
    
    CNNs need training data to learn from examples and scenarios in order to improve their ability to make predictions on new and unseen data. Through analysing a large set of labeled images 
    (in which case is known as a trained dataset), the network recognises general patterns in new images it has never encountered before. An MRI review from the National Library of Medicine states that 
    ‘the success of the ML and DL models is highly dependent on the training data’s quality, quantity and relevance’(Dorfner et al., 2025). 

    <hr></hr>
    
    <div class = "image">
    <img src="table1.png" alt="Banner" width="600" height="300">
    </div>

    <div class = "refheading1">
    <b><em>'Table 1, (Gao et al., 2022) [Test Set Evaluation of Deep Learning System], Jama Network Open'</em></b>
    </div>

     <hr></hr>

    
    This study shows several limitations. The table only has training data obtained from a single centre, meaning it lacks external validation from other hospitals. The number of rare tumours was 
    relatively lower in the training data set which might have caused some bias in the model’s performance. For example, in unseen data sets, most of its decisions will be based on the observations recorded
    in the training data set. Using other MRI views and adding more patient information could help improve its accuracy. In the future, this system could also be developed to assist with treatment planning 
    and predicting patient outcomes.

    <b>2.2 Model Interpretability and clinical trust</b>
      
    There are two main ways in which neural networks make decisions in medical imaging. The first uses attribution-based methods which are flexible with any model. They help show which parts of an MRI/CT image have 
    influenced the model’s prediction and classification. For example, if a neural network predicts that a scan shows a tumour, specific regions of the image are highlighted which contributed to the decision.
    An importance value is assigned to each region of pixels. Red or warm colours show pixels that strongly support the model’s decision whereas blue or cool colours show pixels that oppose or decrease confidence 
    in the prediction. This is shown in Figure 1;

    <hr></hr>

    <div class = "image">
    <img src="fig1.png" alt="Banner" width="600" height="300">
    </div>

    <div class = "refheading2">
    <b><em>'Figure 1, (Huff, Weisman and Jeraj, 2021) [Interpretation and Visualisation Techniques for Deep Learning Models in Medical Imaging], National Library of Medicine'</em></b>
    </div>

     <hr></hr>

    CNNs have proved to be very effective for a variety of medical diagnostic tasks, even sometimes outperforming healthcare professionals. However, the black-box nature has dulled their clinical use. 
    The black box nature of AI refers to systems that have hidden internal decision making processes which are difficult to understand. This raises concerns about trust, fairness and accountability. 
    A peer-reviewed Journal of Imaging stresses that a medical diagnosis system needs to be ‘transparent, understandable and explainable to gain the trust of physicians, regulators and most importantly patients’
    (Singh, Sengupta and Lakshminarayanan, 2020)

    <b>2.3 Poor Generalisation</b>
    
    Several studies have shown that CNNs tend to perform well on one dataset from one hospital, but as soon as sites are exchanged they perform much worse when tested on new data from a different hospital. More caution should be overlooked on small, private datasets as 
    ‘they fail to generalise beyond a single institution or MRI scanner’(Dorfner et al., 2025). This results in unintended bias or spontaneous correlation in the training data. The performance of the model ‘declines dramatically when directly applied to samples from 
    another unknown hospital’(Lu et al., 2022).
    Poor generalisation typically arises because the model is trained on a limited domain without the full representation of the clinical variability, lacking rigorous external validation. An example of this is image variability where tumours differ in size, shape and location. 
    As well as narrow datasets, which means that training on a few images limits the knowledge of patterns the model recognises.

    Moving forward, techniques should be adjusted to enhance domain adaptation. This helps models adjust more easily when new scanners or hospitals are different from the training data - more variety helps the model learn patterns that work everywhere.


    <b>2.4 Ethical and Practical Issues</b>

    CNNs for brain tumour diagnosis face ethical issues like bias and misdiagnosis in algorithms. If these algorithms are developed on datasets that are under- or over-representative population subgroups, they may present bias when deployed in clinical practice, 
    ‘leading to unequal access to care and potential harm to patients’(Khosravi and Schweitzer, 2023). This can be demonstrated through a dataset containing mostly adults, from which the model may misdiagnose children leading to potential harm.

    This leads to slow clinical approval and introduces more boundaries to deep learning methods. This unfairness undermines clinical trust. More examples of ethical and practical issues are listed below:

    <table>

      <tr>
        <th>Ethical Issues</th>
        <th>Practical Issues</th>
      </tr>
      <tr>
        <td>AI mistakes - CNNs can confuse tumour types and execute the wrong diagnosis</td>
        <td>Costly - bulk computing resources are required and it is time consuming to train and run effectively</td>
      </tr>
      <tr>
        <td>Privacy protocol - medical images are sensitive to only individual patient records</td>
        <td>Integration into hospital staff is difficult - gain trust of clinicians and workers, staff also needs to be trained</td>
      </tr>
      <tr>
        <td>Liability and accountability - if a CNN fails to detect a tumour, it's hard to place responsibility on someone</td>
        <td>Model variability - hard to build a model which has good generalisation since every MRI scan has different protocols</td>
      </tr>

      
    </table>


    

    
  </pre>
</body>

  
</html>
