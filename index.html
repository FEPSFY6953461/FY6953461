<!DOCTYPE html>
<html>
<head>

<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">

  
<!-- css -->
  <style>
  .mySlides {display:none;}
  body {
    background-color: #FFFFFF
    margin-left: 50px;
  }
  .ex1 {
    font-family: Calibri, sans-serif;
    text-align: center;
  }
  .heading {
    margin-left: 700px;
  }
  .mainheading {
    font-size: smaller;
  }
  .image {
    margin-left: 700px;
    text-align: center;
  }
    </style>

</head>
  
  


<body>

  
    <!-- image testing -->
<div class = "image">
<img src="WebBanner2023.png" alt="Banner" width="900" height="300">
</div>
  <div class = "mainheading">
  <h1><b>ENG0018 Computer Laboratory 2025-2026</b></h1>
  <h2>Student URN: 6953461</h2>
  </div>
  
  
  <hr>
<h2 style="font-family:calibri;"><b>Conference Project: What are the limitations of CNN approaches in MRI-based brain diagnosis?</b></h2>
<hr>
  
<hr>

   <!-- table of contents -->
  <table>

    <tr>
    <th><h3>Table of contents</h3></th>
    </tr>
    <tr>
    <td><a href="#Abstract">Abstract</td>
    </tr>
    <tr>
    <td><a href="#Introduction">Introduction</td>
    </tr>
    <tr>
    <td><a href="#Analysis/Discussion">Analysis/Dicussion</td>
    </tr>
    <tr>
    <td><a href="#References">References</td>
    </tr>
  </table>

  <hr></hr>

  
  <!-- abstract -->
<p class = "ex1">
  <div class="heading">
  <h3 id="Abstract">Abstract</h3>
  </div>
  <pre>
    This paper argues that there are several limitations surrounding the use of deep learning models in 
    healthcare systems. Convoluted neural networks are now the most communly used tool in tumour diagnosis, 
    classification and medical imaging. The article is evaluated on systemic data and figures obtained by
    private healthcare centres and training data sets from hospitals. We conclude that the main limitations 
    are scarcity of data, clinical trust in model interpretability, poor generalisation and ethical and 
    practical issues
  </pre>
</p>

<hr></hr>

  <!-- intro -->
  <p class = "ex1">
    <div class="heading">
    <h3 id="Introduction">Introduction</h3>
    </div>
    <pre>
      The aetiology of cancer has always been a topic open to scientific and even philosophical debate. 
      Over the centuries, the medicinal world has seen exponential progress in technological advancements, introducing 
      devices such as X-rays and CT/PET scans which have completely reformed medical imaging and in particular tumour diagnosis. 
      MRI is widely used to identify and classify tumours in the brain and other organs. Deep Learning models have become 
      integrated in most hospitals as studies have proved it helps doctors classify tumours faster and more accurately. 
      However, problems such as insufficient data due to patient privacy and inconsistent model performance across hospitals. 
      This review aims to highlight and critically assess the key limitations in deep learning approaches in MRI-based tumour diagnosis. 
      
   </pre>
  </p>

  <!-- analysis/discussion -->
  <p class = "ex1">
    <div class = "heading">
      <h3 id="Discussion">Discussion</h3>
    </div>
  <pre>

    <b>2.1 Scarcity of data</b>
    
    CNNs need training data to learn from examples and scenarios in order to improve their ability to make predictions on new and unseen data. Through analysing a large set of labeled images 
    (in which case is known as a trained dataset), the network recognises general patterns in new images it has never encountered before. An MRI review from the National Library of Medicine states that 
    ‘the success of the ML and DL models is highly dependent on the training data’s quality, quantity and relevance’(Dorfner et al., 2025). 

    <hr></hr>
    
    <div class = "image">
    <img src="table1.png" alt="Banner" width="600" height="300">
    </div>

    <hr></hr>

    <div class = "mainheading">
    <b><em>'Table 1, (Gao et al., 2022) [Test Set Evaluation of Deep Learning System], Jama Network Open'</em></b>
    </div>
    
    This study shows several limitations. The table only has training data obtained from a single centre, meaning it lacks external validation from other hospitals. The number of rare tumours was 
    relatively lower in the training data set which might have caused some bias in the model’s performance. For example, in unseen data sets, most of its decisions will be based on the observations recorded
    in the training data set. Using other MRI views and adding more patient information could help improve its accuracy. In the future, this system could also be developed to assist with treatment planning 
    and predicting patient outcomes.

    <b>2.2 Model Interpretability and clinical trust</b>
      
    There are two main ways in which neural networks make decisions in medical imaging. The first uses attribution-based methods which are flexible with any model. They help show which parts of an MRI/CT image have 
    influenced the model’s prediction and classification. For example, if a neural network predicts that a scan shows a tumour, specific regions of the image are highlighted which contributed to the decision.
    An importance value is assigned to each region of pixels. Red or warm colours show pixels that strongly support the model’s decision whereas blue or cool colours show pixels that oppose or decrease confidence 
    in the prediction. This is shown in Figure 1;

    <hr></hr>

    <div class = "image">
    <img src="fig1.png" alt="Banner" width="600" height="300">
    </div>

    <hr></hr>

    <div class = "mainheading">
    <b><em>'Figure 1, (Huff, Weisman and Jeraj, 2021) [Interpretation and Visualisation Techniques for Deep Learning Models in Medical Imaging], National Library of Medicine'</em></b>
    </div>
    
    CNNs have proved to be very effective for a variety of medical diagnostic tasks, even sometimes outperforming healthcare professionals. However, the black-box nature has dulled their clinical use. 
    The black box nature of AI refers to systems that have hidden internal decision making processes which are difficult to understand. This raises concerns about trust, fairness and accountability. 
    A peer-reviewed Journal of Imaging stresses that a medical diagnosis system needs to be ‘transparent, understandable and explainable to gain the trust of physicians, regulators and most importantly patients’
    (Singh, Sengupta and Lakshminarayanan, 2020)

    <b>2.3 Poor Generalisation</b>
    
    Several studies have shown that CNNs tend to perform well on one dataset from one hospital, but as soon as sites are exchanged they perform much worse when tested on new data from a different hospital. More caution should be overlooked on small, private datasets as 
    ‘they fail to generalise beyond a single institution or MRI scanner’(Dorfner et al., 2025). This results in unintended bias or spontaneous correlation in the training data. The performance of the model ‘declines dramatically when directly applied to samples from 
    another unknown hospital’(Lu et al., 2022).
    Poor generalisation typically arises because the model is trained on a limited domain without the full representation of the clinical variability, lacking rigorous external validation. 
    
  </pre>

</body>

  
</html>
